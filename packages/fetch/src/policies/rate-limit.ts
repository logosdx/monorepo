import { RateLimitTokenBucket, RateLimitError } from '@logosdx/utils';

import type {
    _InternalHttpMethods,
    RateLimitRule,
    RateLimitConfig,
    RequestSerializer,
    CacheAdapter,
    RequestKeyOptions
} from '../types.ts';

import { ResiliencePolicy } from './base.ts';
import { endpointSerializer } from '../serializers/index.ts';
import { validateMatchRules } from '../helpers.ts';


/**
 * Execution context for rate limit guard.
 * Provides dependencies needed to execute rate limiting logic.
 */
export interface RateLimitExecutionContext<S, H, P> {

    /** HTTP method */
    method: string;

    /** Request path */
    path: string;

    /** Full normalized request options (used for serializer and events) */
    normalizedOpts: RequestKeyOptions<S, H, P>;

    /** AbortController for cancellation */
    controller: AbortController;

    /** Emit an event */
    emit: (event: string, data: unknown) => void;

    /** Clear any pending timeout */
    clearTimeout: () => void;

    /** Factory for creating abort errors */
    createAbortError: (message: string) => Error;
}


/**
 * Default HTTP methods for rate limiting.
 * All methods are rate limited by default.
 */
const DEFAULT_RATELIMIT_METHODS: _InternalHttpMethods[] = [
    'GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'OPTIONS'
];

/**
 * Default max calls per window.
 */
const DEFAULT_MAX_CALLS = 100;

/**
 * Default window duration in milliseconds (1 minute).
 */
const DEFAULT_WINDOW_MS = 60000;


/**
 * Extended internal state for rate limit policy.
 * Includes rate limit-specific fields and token bucket management.
 */
export interface RateLimitPolicyState<S, H, P> {

    /** Whether the policy is globally enabled */
    enabled: boolean;

    /** Set of HTTP methods this policy applies to */
    methods: Set<string>;

    /** The serializer function for bucket key generation */
    serializer: RequestSerializer<S, H, P>;

    /** Memoized rule cache: method:path -> resolved rule or null */
    rulesCache: Map<string, RateLimitRule<S, H, P> | null>;

    /** Max calls per window */
    maxCalls: number;

    /** Window duration in milliseconds */
    windowMs: number;

    /** Whether to wait for token vs reject immediately */
    waitForToken: boolean;

    /** Token buckets by key */
    rateLimiters: Map<string, RateLimitTokenBucket>;
}


/**
 * Rate limit policy for controlling request rate.
 *
 * Uses token bucket algorithm to enforce rate limits. Each unique key
 * (generated by the serializer) gets its own bucket, allowing per-endpoint
 * or per-user rate limiting.
 *
 * Uses endpoint-scoped serialization by default (method + path),
 * meaning all requests to the same endpoint share a rate limit bucket
 * regardless of their parameters or payload.
 *
 * @template S - Instance state type
 * @template H - Headers type
 * @template P - Params type
 *
 * @example
 * ```typescript
 * const rateLimitPolicy = new RateLimitPolicy<State, Headers, Params>();
 *
 * rateLimitPolicy.init({
 *     enabled: true,
 *     maxCalls: 60,
 *     windowMs: 60000,  // 60 req/min
 *     rules: [
 *         { startsWith: '/api/search', maxCalls: 10, windowMs: 1000 },  // 10/sec
 *         { startsWith: '/admin', enabled: false }  // No limit for admin
 *     ]
 * });
 * ```
 */
export class RateLimitPolicy<
    S = unknown,
    H = unknown,
    P = unknown
> extends ResiliencePolicy<RateLimitConfig<S, H, P>, RateLimitRule<S, H, P>, S, H, P> {

    /**
     * Extended state with rate limit-specific fields.
     * Note: We override the base state type to include rate limit-specific fields.
     */
    protected state: RateLimitPolicyState<S, H, P> | null = null;

    /**
     * Adapter for distributed rate limiting.
     */
    #adapter: CacheAdapter<unknown> | undefined;

    /**
     * Get the adapter (if configured).
     */
    get adapter(): CacheAdapter<unknown> | undefined {

        return this.#adapter;
    }

    /**
     * Get the default serializer for rate limiting.
     * Uses endpoint-scoped serialization (method + path).
     */
    protected getDefaultSerializer(): RequestSerializer<S, H, P> {

        return endpointSerializer as RequestSerializer<S, H, P>;
    }

    /**
     * Get the default HTTP methods for rate limiting.
     * All methods are rate limited by default.
     */
    protected getDefaultMethods(): _InternalHttpMethods[] {

        return DEFAULT_RATELIMIT_METHODS;
    }

    /**
     * Initialize the rate limit policy with configuration.
     *
     * Extends base init to handle rate limit-specific fields.
     */
    init(config?: boolean | RateLimitConfig<S, H, P>): void {

        if (!config) {

            this.state = null;
            this.config = null;
            this.#adapter = undefined;
            return;
        }

        if (config === true) {

            this.state = {
                enabled: true,
                methods: new Set(this.getDefaultMethods()),
                serializer: this.getDefaultSerializer(),
                rulesCache: new Map(),
                maxCalls: DEFAULT_MAX_CALLS,
                windowMs: DEFAULT_WINDOW_MS,
                waitForToken: true,
                rateLimiters: new Map()
            };
            this.config = {} as RateLimitConfig<S, H, P>;
            this.#adapter = undefined;
            return;
        }

        this.config = config;
        this.#adapter = (config as any).adapter;  // adapter support from design doc

        this.state = {
            enabled: config.enabled !== false,
            methods: new Set(config.methods ?? this.getDefaultMethods()),
            serializer: config.serializer ?? this.getDefaultSerializer(),
            rulesCache: new Map(),
            maxCalls: config.maxCalls ?? DEFAULT_MAX_CALLS,
            windowMs: config.windowMs ?? DEFAULT_WINDOW_MS,
            waitForToken: config.waitForToken ?? true,
            rateLimiters: new Map()
        };

        if (config.rules) {

            validateMatchRules(config.rules);
        }
    }

    /**
     * Merge a matched rule with policy defaults.
     * Includes rate limit-specific fields (maxCalls, windowMs, waitForToken).
     */
    protected mergeRuleWithDefaults(rule: RateLimitRule<S, H, P> | null): RateLimitRule<S, H, P> {

        if (!this.state) {

            return {
                enabled: true,
                serializer: this.getDefaultSerializer(),
                maxCalls: DEFAULT_MAX_CALLS,
                windowMs: DEFAULT_WINDOW_MS,
                waitForToken: true
            };
        }

        return {
            enabled: true,
            serializer: rule?.serializer ?? this.state.serializer,
            maxCalls: rule?.maxCalls ?? this.state.maxCalls,
            windowMs: rule?.windowMs ?? this.state.windowMs,
            waitForToken: rule?.waitForToken ?? this.state.waitForToken
        };
    }

    /**
     * Resolve rate limit configuration for a request.
     *
     * Convenience method that wraps the base `resolve()` with the
     * policy-specific skip callback.
     */
    resolveForRequest(
        method: string,
        path: string,
        ctx: RequestKeyOptions<S, H, P>
    ): RateLimitRule<S, H, P> | null {

        const skipCallback = this.config?.shouldRateLimit
            ? (c: RequestKeyOptions<S, H, P>) => this.config!.shouldRateLimit!(c) === false
            : undefined;

        return this.resolve(method, path, ctx, skipCallback);
    }

    /**
     * Get or create a rate limiter for the given key.
     *
     * Rate limiters are cached by key to ensure all requests to the same
     * endpoint share the same token bucket.
     *
     * @param key - The bucket key (from serializer)
     * @param maxCalls - Max calls per window
     * @param windowMs - Window duration in milliseconds
     * @returns The token bucket for this key
     */
    getRateLimiter(key: string, maxCalls: number, windowMs: number): RateLimitTokenBucket {

        if (!this.state) {

            throw new Error('Rate limiting not initialized');
        }

        let bucket = this.state.rateLimiters.get(key);

        if (!bucket) {

            // Token bucket: capacity and time per token
            // If maxCalls=100 and windowMs=60000, we want 100 requests per minute
            // So refillIntervalMs = windowMs / maxCalls = 600ms per token
            const refillIntervalMs = windowMs / maxCalls;
            bucket = new RateLimitTokenBucket({ capacity: maxCalls, refillIntervalMs });

            this.state.rateLimiters.set(key, bucket);
        }

        return bucket;
    }

    /**
     * Get the onRateLimit callback from config.
     */
    get onRateLimit(): RateLimitConfig<S, H, P>['onRateLimit'] {

        return this.config?.onRateLimit;
    }

    /**
     * Execute rate limit guard for a request.
     *
     * Resolves config, checks token availability, and either:
     * - Returns immediately if token is available
     * - Waits for token if waitForToken is true
     * - Throws RateLimitError if waitForToken is false
     *
     * @param ctx - Execution context with dependencies
     * @throws RateLimitError if rate limit exceeded and waitForToken is false
     * @throws Error if request is aborted while waiting
     */
    async executeGuard(ctx: RateLimitExecutionContext<S, H, P>): Promise<void> {

        const { method, path, normalizedOpts, controller, emit, clearTimeout, createAbortError } = ctx;

        const config = this.resolveForRequest(method, path, normalizedOpts);

        if (config === null) {

            return;
        }

        const key = config.serializer!(normalizedOpts);
        const bucket = this.getRateLimiter(key, config.maxCalls!, config.windowMs!);

        const snapshot = bucket.snapshot;
        const waitTimeMs = bucket.getWaitTimeMs(1);

        // Build event data once for reuse
        const eventData = {
            ...normalizedOpts,
            key,
            currentTokens: snapshot.currentTokens,
            capacity: snapshot.capacity,
            waitTimeMs,
            nextAvailable: bucket.getNextAvailable(1),
        };

        if (waitTimeMs > 0) {

            // Rate limit exceeded - need to wait or reject
            if (!config.waitForToken) {

                // Reject immediately
                emit('fetch-ratelimit-reject', eventData);
                clearTimeout();

                throw new RateLimitError(
                    `Rate limit exceeded for ${key}. Try again in ${waitTimeMs}ms`,
                    config.maxCalls!
                );
            }

            // Wait for token
            emit('fetch-ratelimit-wait', eventData);

            // Call the onRateLimit callback if configured
            if (this.onRateLimit) {

                await this.onRateLimit(normalizedOpts, waitTimeMs);
            }

            // Wait and consume atomically, respecting abort signal
            const acquired = await bucket.waitAndConsume(1, {
                abortController: controller,
            });

            if (!acquired) {

                // Aborted while waiting
                clearTimeout();
                throw createAbortError('Request aborted while waiting for rate limit');
            }

            // Token acquired after waiting
            const postWaitSnapshot = bucket.snapshot;

            emit('fetch-ratelimit-acquire', {
                ...normalizedOpts,
                key,
                currentTokens: postWaitSnapshot.currentTokens,
                capacity: postWaitSnapshot.capacity,
                waitTimeMs: 0,
                nextAvailable: bucket.getNextAvailable(1),
            });
        }
        else {

            // Token available immediately - consume it
            bucket.consume(1);

            // Get post-consumption snapshot for event data
            const postConsumeSnapshot = bucket.snapshot;

            emit('fetch-ratelimit-acquire', {
                ...normalizedOpts,
                key,
                currentTokens: postConsumeSnapshot.currentTokens,
                capacity: postConsumeSnapshot.capacity,
                waitTimeMs: 0,
                nextAvailable: bucket.getNextAvailable(1),
            });
        }
    }
}
